# Daily Progress - November 5, 2025

## Project: Decentralized Peer-to-Peer Storage System

### Overview

Today's work focused on implementing network-wide file deletion functionality. Previously, the delete command only removed files from the local node. Now, when a file is deleted, it is automatically removed from all connected peers in the network, ensuring consistency across the distributed storage system. This brings the delete operation in line with the store operation, which already broadcasts to all peers.

### Problem Statement

**Initial State:**
- Delete command only removed files from local storage
- Files stored on remote peers remained after deletion
- Inconsistent state across the network
- Users could retrieve "deleted" files from other nodes

**Requirements:**
- Delete files from all connected peers, not just local node
- Maintain consistency across the distributed network
- Handle both locally-stored and peer-received files
- Ensure reliable message delivery

### Key Changes

#### 1. Delete Message Type

**Implementation:**
```go
type MessageDeleteFile struct {
    Key string
}
```

**Design Choice: Hash-Based Key**
The `Key` field contains the hashed key (using `hashKey()`), not the original key name. This design choice was made for several reasons:

1. **Consistency**: Matches the pattern used in `MessageStoreFile` and `MessageGetFile`, which also use hashed keys
2. **Privacy**: Hash provides some level of obfuscation of the original filename
3. **Size**: Hash is fixed length (MD5 = 32 hex chars), making message size predictable
4. **Efficiency**: Hash lookup is faster than string comparison in some cases

**Trade-off Consideration:**
The downside is that receivers need to look up the original key from the database. However, this maintains consistency with the existing message protocol and keeps messages compact.

#### 2. Message Registration

**Implementation:**
```go
func init() {
    gob.Register(MessageStoreFile{})
    gob.Register(MessageGetFile{})
    gob.Register(MessageDeleteFile{})  // Added
}
```

**Design Choice: Gob Registration**
Using `gob.Register()` allows the `Message` struct to use `any` type for its `Payload` field while still being serializable. This pattern:
- Provides type safety through the switch statement in `handleMessage()`
- Allows extensibility (new message types can be added easily)
- Maintains backward compatibility (existing code continues to work)

**Why Not Use Interfaces?**
We could have used an interface (e.g., `MessagePayload`), but gob registration was chosen because:
- It's simpler - no need to define interfaces
- Works well with the `any` type pattern
- Matches existing code style in the project

#### 3. FileServer.Delete() Method

**Implementation Strategy:**
```go
func (s *FileServer) Delete(key string) error {
    // 1. Delete locally first
    // 2. Broadcast delete message to all peers
    // 3. Return success
}
```

**Design Choice: Optimistic Local Deletion**
We delete locally first, then broadcast. This approach:
- **User Experience**: User sees immediate feedback (local deletion succeeds even if broadcast fails)
- **Idempotency**: If broadcast fails, user can retry; local deletion won't fail on retry
- **Performance**: Local operation is fast, network operation is async

**Alternative Consideration:**
We could broadcast first, then delete locally after confirmation. However, this would:
- Increase latency (user waits for network round-trips)
- Add complexity (need to handle partial failures)
- Reduce user experience (slower feedback)

**Design Choice: Always Broadcast**
Even if the file doesn't exist locally, we still broadcast the delete message. This handles the case where:
- File was deleted locally but still exists on peers
- User wants to ensure cleanup across the network
- Network state is inconsistent

#### 4. Delete Message Handler

**Implementation:**
```go
func (s *FileServer) handleMessageDeleteFile(from string, msg MessageDeleteFile) error {
    // 1. Try to find original key from database
    // 2. If found, delete using original key
    // 3. If not found, try deleting using hashed key
    // 4. Handle gracefully if file doesn't exist
}
```

**Design Choice: Dual-Key Strategy**
The handler uses a two-step approach to handle different file storage scenarios:

1. **Locally Stored Files**: Files stored with the original key have metadata in the database. The handler looks up the original key by hash and deletes using that key.

2. **Peer-Received Files**: Files received from peers were stored with the hashed key (because `handleMessageStoreFile` receives `msg.Key` which is already hashed). The handler tries to delete directly using the hashed key.

**Why This Complexity?**
This is necessary because of how files are stored:
- **Local storage**: `Store()` uses original key → `PathTransformFunc` hashes it → file stored at hash path
- **Peer storage**: `handleMessageStoreFile()` receives hashed key → `PathTransformFunc` hashes it again → file stored at double-hash path

**Design Alternative Considered:**
We could have included the original key name in `MessageStoreFile` and `MessageDeleteFile`. However, this would:
- Increase message size
- Break backward compatibility
- Require changes to existing message types

The current approach maintains backward compatibility while handling both cases.

**Design Choice: Graceful Degradation**
If the file doesn't exist locally, the handler logs a message and returns nil (no error). This is because:
- File might have been deleted already
- File might not exist on this particular node
- Network might be in inconsistent state
- We don't want to fail the entire operation for one node

#### 5. Connection Management

**Problem Identified:**
Initial implementation had issues where delete was called before peer connections were established, resulting in "Connected to 0 peer(s)" and failed broadcasts.

**Solution: waitForPeers() Method**

**Implementation:**
```go
func (s *FileServer) waitForPeers(timeout time.Duration) error {
    deadline := time.Now().Add(timeout)
    for time.Now().Before(deadline) {
        s.peersLock.Lock()
        peerCount := len(s.peers)
        s.peersLock.Unlock()
        
        if peerCount > 0 {
            return nil
        }
        time.Sleep(100 * time.Millisecond)
    }
    return fmt.Errorf("timeout waiting for peer connections")
}
```

**Design Choice: Polling vs Events**
We chose polling over an event-based approach because:
- **Simplicity**: No need for channels or callbacks
- **Reliability**: Polling is straightforward and easy to debug
- **Low Overhead**: 100ms polling interval is acceptable for CLI commands
- **Timeout Safety**: Built-in timeout prevents infinite waiting

**Design Choice: 5-Second Timeout**
The timeout is set to 5 seconds, which:
- Is long enough for typical network connections to establish
- Is short enough that users won't wait too long
- Provides good balance for CLI commands (not a long-running service)

**Design Choice: Non-Blocking**
If timeout occurs, we log a warning but proceed anyway. This ensures:
- User isn't blocked if network is slow
- Command can still delete locally even if peers aren't connected
- Better user experience (partial success vs failure)

#### 6. Thread Safety in Broadcast

**Problem:**
The `broadcast()` function iterates over `s.peers` without locking, which could cause race conditions if peers are added/removed concurrently.

**Solution:**
```go
func (s *FileServer) broadcast(msg *Message) error {
    // ... encode message ...
    
    s.peersLock.Lock()
    defer s.peersLock.Unlock()
    
    for addr, peer := range s.peers {
        // ... send to peer ...
    }
}
```

**Design Choice: Coarse-Grained Locking**
We lock the entire iteration rather than per-peer locking because:
- **Simplicity**: Easier to understand and maintain
- **Consistency**: Ensures atomic snapshot of peers map
- **Performance**: Lock overhead is minimal for typical peer counts (< 10)
- **Correctness**: Prevents peers map from changing during iteration

**Design Choice: Lock Duration**
We hold the lock for the entire iteration, which is acceptable because:
- Network I/O (`peer.Send()`) is fast for small messages
- Typical peer count is small
- Prevents complex lock ordering issues

#### 7. Command Updates

**Updated Commands:**
- `delete`: Now uses `FileServer.Delete()` instead of `s.store.Delete()`
- `store`: Added `waitForPeers()` call for consistency
- `get`: Added `waitForPeers()` call for consistency

**Design Choice: Consistent Pattern**
All file operation commands now wait for peer connections. This ensures:
- **Reliability**: Operations succeed when peers are available
- **Consistency**: All commands behave predictably
- **User Experience**: Users get consistent behavior across commands

**Design Choice: Conditional Waiting**
We only wait for peers if bootstrap nodes are specified:
```go
if len(bootstrap) > 0 {
    if err := s.waitForPeers(5 * time.Second); err != nil {
        fmt.Printf("Warning: %v. Proceeding anyway.\n", err)
    }
}
```

This design choice:
- Avoids unnecessary waiting when no peers are expected
- Allows commands to work in standalone mode
- Provides flexibility for different use cases

### Technical Implementation Details

#### Message Flow

```
User runs: delete <key>
    ↓
FileServer.Delete(key)
    ↓
1. Delete locally (s.store.Delete(key))
    ↓
2. Broadcast MessageDeleteFile{Key: hashKey(key)}
    ↓
3. Each peer receives message
    ↓
4. handleMessageDeleteFile() called
    ↓
5. Look up original key from database
    ↓
6. Delete file using appropriate key
```

#### Key Resolution Algorithm

```
1. Check if database exists
   ↓
2. Query database for file with matching hash
   ↓
3. If found:
   - Use original key name
   - Delete using original key
   ↓
4. If not found:
   - Try deleting using hashed key directly
   - This handles peer-received files
```

#### Error Handling Strategy

**Graceful Degradation:**
- If file doesn't exist locally: Log and continue
- If database lookup fails: Try hashed key approach
- If peer connection fails: Log warning, continue
- If broadcast fails: Return error (user can retry)

**Design Philosophy:**
Failures are logged but don't stop the operation unless critical. This ensures:
- Partial success is better than total failure
- Users can see what happened through logs
- Network inconsistencies don't break the system

### Testing and Validation

**Test Scenario:**
1. Start node A on :3000
2. Start node B on :4000, connect to :3000
3. Store file on node B
4. Verify file exists on both nodes
5. Delete file from node B
6. Verify file is deleted from both nodes

**Results:**
- ✅ File successfully deleted from local node
- ✅ Delete message broadcast to connected peers
- ✅ File successfully deleted from remote peers
- ✅ File cannot be retrieved after deletion

### Challenges Overcome

#### Challenge 1: Connection Timing
**Problem:** Delete was called before peer connections were established.

**Solution:** Added `waitForPeers()` method with timeout.

**Design Decision:** Polling with timeout rather than event-based because:
- Simpler implementation
- Works well for CLI commands
- No need for complex event handling

#### Challenge 2: Key Resolution
**Problem:** Files stored locally vs. received from peers use different keys.

**Solution:** Dual-key lookup strategy (database lookup + hashed key fallback).

**Design Decision:** Two-step approach rather than changing message format because:
- Maintains backward compatibility
- Handles both cases correctly
- No breaking changes to existing code

#### Challenge 3: Thread Safety
**Problem:** Race conditions when accessing peers map during broadcast.

**Solution:** Added mutex locking in `broadcast()` method.

**Design Decision:** Coarse-grained locking rather than fine-grained because:
- Simpler to implement
- Sufficient for typical use cases
- Prevents complex lock ordering issues

### Benefits of This Implementation

1. **Consistency**: Files are removed from all nodes, preventing orphaned data
2. **Reliability**: Proper connection handling ensures messages are delivered
3. **Flexibility**: Handles both local and peer-stored files correctly
4. **User Experience**: Single delete command removes files network-wide
5. **Backward Compatibility**: No breaking changes to existing functionality
6. **Graceful Degradation**: System continues to work even with partial failures

### Design Principles Applied

1. **Consistency**: Delete operation follows same pattern as Store operation
2. **Simplicity**: Solutions are straightforward and easy to understand
3. **Reliability**: Error handling ensures system continues to work
4. **User Experience**: Operations complete quickly with clear feedback
5. **Maintainability**: Code is well-structured and documented
6. **Backward Compatibility**: Changes don't break existing functionality

### Future Improvements

Potential enhancements identified:

1. **Delete Acknowledgments**: Peers could send confirmation messages
2. **Retry Logic**: Automatically retry failed broadcasts
3. **Tombstone Markers**: Mark deleted files to prevent re-creation
4. **Partial Delete Tracking**: Track which peers successfully deleted
5. **Event-Based Connection Waiting**: Use channels instead of polling
6. **Original Key in Messages**: Include original key to simplify lookup

### Code Metrics

**Lines Added:**
- `MessageDeleteFile` type: 3 lines
- `handleMessageDeleteFile()`: 45 lines
- `FileServer.Delete()`: 35 lines
- `waitForPeers()`: 14 lines
- Message registration: 1 line
- Handler case: 3 lines
- **Total**: ~101 lines

**Complexity:**
- Cyclomatic complexity remains low (each function is straightforward)
- No new dependencies introduced
- Maintains existing code patterns

### Lessons Learned

1. **Connection Management**: Always wait for connections before network operations
2. **Key Management**: Hash-based keys require careful handling of original vs. hashed
3. **Thread Safety**: Concurrent access to shared data structures requires proper locking
4. **Error Handling**: Graceful degradation provides better UX than strict failure
5. **Testing**: Multi-node testing is essential for distributed systems

### Conclusion

This implementation successfully adds network-wide file deletion to the P2P storage system. The design choices prioritize simplicity, reliability, and backward compatibility while handling the complexities of distributed file storage. The dual-key lookup strategy elegantly handles both local and peer-stored files without requiring breaking changes to the message protocol.

The system now provides complete CRUD operations (Create via Store, Read via Get, Delete) with proper network synchronization, making it a fully functional distributed storage system.

