## Market Research

This project was as much an educational journey as a software build. My guiding philosophy was to strip away layers of abstraction and reconstruct the essential components of a decentralized storage system from first principles. I wanted to demystify the “magic” that makes large-scale platforms work by re-implementing storage and networking primitives myself. Instead of surveying the market to find a competitive niche, I studied existing systems to inform the ground-up design choices that would challenge me and expand my understanding.

IPFS became the central conceptual blueprint for my exploration. Its design showed me how decentralized storage can be both practical and elegant. Content-addressable storage was the first major insight I drew from it. CAS makes every piece of data self-verifying because the identifier is a cryptographic hash of the content. That property delivers integrity checks for free: if a bit flips, the hash no longer matches. It also enables automatic deduplication because identical content collapses to a single address. Recognising this was a turning point. Importantly, I did not just adopt the idea in theory. I chose to implement a custom CAS layer: hashing files with SHA-256, arranging them in a directory structure derived from the hash prefix, and writing retrieval logic that reconstructs paths and ensures the data returned matches the expected digest. Building that pipeline by hand gave me a deeper intuition for why CAS is such a powerful primitive in distributed systems.

At the same time, IPFS showed me where deliberate simplification made sense. Its networking layer operates on a global scale, combining a public Kademlia DHT, sophisticated NAT traversal, and a host of application-level protocols layered over libp2p. That stack solves hard problems for an untrusted, worldwide network, but it was misaligned with my goals. For a small, private group, the overhead of joining a public DHT is unnecessary; for my learning objectives, it would offload the most interesting transport problems to someone else’s library. By consciously deciding not to mirror IPFS verbatim, I carved out a scope where I could focus on the mechanics that mattered most to me.

The biggest crossroads came when I examined libp2p, IPFS’s networking substrate. On paper, libp2p was compelling. Its modular architecture covers peer routing, NAT traversal, secure channels, multiplexing, and pub/sub, all battle-tested by production deployments. Leveraging it could have delivered a mature P2P stack quickly. I realised, however, that doing so conflicted with my first-principles philosophy. Relying on libp2p would make me a consumer of a P2P library rather than a builder of a P2P system. I would spend my time wiring up APIs instead of grappling with how peers actually exchange bytes over a transport. Additionally, the library’s breadth would introduce significant integration complexity—configuring transports, managing peer stores, and handling protocol negotiations—risking a diversion from the core storage features I wanted to implement. The abstraction promised convenience, but it threatened to obscure the very lessons I sought.

Rejecting libp2p meant embracing native TCP sockets and owning every piece of the networking stack. That choice forced me to solve granular problems that the library would have hidden. Message framing became an immediate challenge: raw TCP is a continuous stream, so I had to design a protocol that prefixes each payload with its length to delineate messages reliably. Peer discovery demanded another custom solution. Without a DHT, new nodes need a way to learn about the network, so I implemented a bootstrap mechanism where known peers share their address book with newcomers. Serialization was equally instructive. The system exchanges different message types—file storage requests, retrieval responses, peer lists—so I wrote encoding and decoding routines to translate Go structs into byte slices and back. Wrestling with these details turned out to be the most valuable learning experience of the entire project. Every bug, from mismatched message lengths to peers timing out during handshakes, reinforced my understanding of how distributed applications behave at the transport layer.

Researching data availability strategies led me to evaluate approaches like erasure coding in Storj and the chunking-plus-swarm model popularised by BitTorrent. These methods deliver excellent storage efficiency and resilience across massive, semi-trusted networks. Implementing them, however, would have required substantial additional infrastructure: sophisticated coordination for fragment placement, reassembly logic, and fault tolerance algorithms tuned for large populations. After weighing those complexities, I chose full replication. Every peer stores the entire encrypted file. This decision aligned with my build-from-scratch ethos because it kept the implementation tractable while ensuring the system stayed reliable. For the target scenario—small groups where storage capacity is less critical than availability and clarity—the trade-off is sensible. The network remains resilient to peer churn without introducing opaque machinery that would dilute the project’s educational value.

Synthesising this research convinced me that the project’s worth lies not in competing with established decentralized platforms, but in demonstrating how to reconstruct their core ideas in a simpler, targeted context. Studying IPFS and libp2p informed my architecture, yet I rebuilt the pieces to understand them intimately. The resulting system shows that a ground-up implementation can serve the needs of a specific niche: small, privacy-conscious teams that value transparency and control. The journey from theory to practice, rather than market differentiation, is the narrative that drives this work forward.

